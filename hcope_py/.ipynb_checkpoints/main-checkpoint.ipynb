{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from data_loader import data_extractor\n",
    "from hcope import Hcope\n",
    "from mdp.environments import cartpole as cp\n",
    "from mdp.history import History\n",
    "from mdp.policies.f_policy import FBSoftmax\n",
    "from policy_evaluation import CartPoleEvaluation\n",
    "import hcope\n",
    "\n",
    "MODE = 1  # 0 for self generated; 1 for submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 1 [ 0.01 -0.01  1.    1.  ] 200000\n",
      "(4_w,8)-aCMA-ES (mu_w=2.6,w_1=52%) in dimension 4 (seed=352037, Thu Dec 12 15:29:21 2019)\n",
      "1,1,1,1 6.865028636019511\n",
      "Trying 0th policy\n",
      "getting new theta\n",
      "[999998.6944716283, 999995.387933059, 999998.6400929468, 999995.9201100421, 999996.1133422112, 999997.054108603, 999998.1349745641, 999996.1470423178]\n",
      "told\n",
      "8.588520517205238\n",
      "Trying 1th policy\n",
      "getting new theta\n",
      "[999995.2467213145, 999995.6452498795, 999995.3310112935, 999995.5548666166, 999995.4418982789, 999995.7651552833, 999995.4996350876, 999997.2459294413]\n",
      "told\n",
      "6.317136329807672\n",
      "Trying 2th policy\n",
      "getting new theta\n",
      "[999995.9526810754, 999995.7144620351, 999995.9950494824, 999995.3815933557, 999996.2181848341, 999995.1895317418, 999995.1157301898, 999997.2789890975]\n",
      "told\n",
      "6.040906535893012\n",
      "Trying 3th policy\n",
      "getting new theta\n",
      "[999995.8877297473, 999998.3013670209, 999995.3174832866, 999995.8207064067, 999995.2002740641, 999996.1212072159, 999996.1170181883, 999996.501441665]\n",
      "told\n",
      "7.551892860191\n",
      "Trying 4th policy\n",
      "getting new theta\n",
      "[999995.4737358089, 999995.4787016761, 999995.3292727266, 999995.9124221876, 999996.6679032822, 999995.5487453126, 999995.4947525067, 999995.9208326546]\n",
      "told\n",
      "7.711818091103473\n",
      "Trying 5th policy\n",
      "getting new theta\n",
      "[999996.2846863272, 999995.943991024, 999995.3810170647, 999995.3008597932, 999995.492515516, 999996.4078512845, 999995.4806137665, 999994.9818734648]\n",
      "told\n",
      "5.027282307323789\n",
      "Trying 6th policy\n",
      "getting new theta\n",
      "[999995.7262109156, 999995.4608180854, 999995.2747525878, 999995.63241572, 999995.2232518346, 999995.421712522, 999996.0340651389, 999995.5050812273]\n",
      "told\n",
      "7.002378681863338\n",
      "Trying 7th policy\n",
      "getting new theta\n",
      "[999995.4879309634, 999995.3275247413, 999996.0683395774, 999995.3038741277, 999994.9201680515, 999995.6740251366, 999995.7424474828, 999995.2751212217]\n",
      "told\n",
      "4.781694402221181\n",
      "Trying 8th policy\n",
      "getting new theta\n",
      "[999995.242502729, 999995.2090445227, 999995.289561031, 999994.9311872683, 999995.0616074785, 999995.4021619207, 999995.4985873738, 999995.3235263157]\n",
      "told\n",
      "5.160185147908908\n",
      "Trying 9th policy\n",
      "getting new theta\n",
      "[999994.9230412808, 999994.8240539521, 999994.9402834461, 999994.7739982157, 999995.2010356195, 999994.8776941615, 999995.5665167349, 999995.4424414937]\n",
      "told\n",
      "3.9912509874429083\n"
     ]
    }
   ],
   "source": [
    "MODE = 1\n",
    "if MODE == 1:\n",
    "    num_state_features, num_actions, k, theta_b, num_episodes, states, actions, rewards, pi_b_St_At = data_extractor(\"../data/test_data/data.csv\")\n",
    "    print(num_state_features, num_actions, k, theta_b, num_episodes)\n",
    "    D = []\n",
    "    \n",
    "    num_episodes = 10\n",
    "    for i in range(num_episodes):\n",
    "        history = History(states[i], actions[i], rewards[i])\n",
    "        D.append(history)\n",
    "    \n",
    "    J_pi_b_hat = 0.0\n",
    "    for H in D:\n",
    "        J_pi_b_hat += np.sum(H.rewards)\n",
    "    \n",
    "    J_pi_b_hat /= len(D)\n",
    "    \n",
    "    policy = FBSoftmax(num_state_features,num_actions,k,0)\n",
    "    policy.parameters = theta_b\n",
    "    \n",
    "    hc = Hcope(D, policy, int(len(D)*0.5), J_pi_b_hat)\n",
    "    test_policy = FBSoftmax(num_state_features,num_actions,k,0)\n",
    "    test_policy.parameters = np.array([1,1,1,1])\n",
    "    ret = hc.test_pdis(test_policy)\n",
    "    print(\"1,1,1,1\", ret)\n",
    "    \n",
    "#     J_pi_b_hat = 0.0\n",
    "#     for H in D:\n",
    "#         J_pi_b_hat += np.sum(H.rewards)\n",
    "    \n",
    "#     J_pi_b_hat /= len(D)\n",
    "    \n",
    "#     for policy_number in range(10):\n",
    "        \n",
    "#         print(\"Trying \" + str(policy_number) + \"th policy\")\n",
    "    \n",
    "#         policy = FBSoftmax(num_state_features,num_actions,k,0)\n",
    "#         theta = hc.sample_theta_c()\n",
    "\n",
    "#         test_policy = FBSoftmax(num_state_features,num_actions,k,0)\n",
    "#         test_policy.parameters = theta\n",
    "#         ret = hc.test_pdis(test_policy)\n",
    "#         print(ret)\n",
    "#     #     hc.plot_es()\n",
    "    \n",
    "elif MODE == 0:\n",
    "    policy = FBSoftmax(4,2,1,0)\n",
    "    numEpisode = 1000\n",
    "\n",
    "    environment = cp.Cartpole\n",
    "\n",
    "    t = tabp_carp_evaluation(policy, environment)\n",
    "\n",
    "    mean_J_hat, D = t.run_policy(np.ones((2,5)), numEpisode)\n",
    "\n",
    "    print(\"Average J:\"+ str(mean_J_hat))\n",
    "\n",
    "     \n",
    "    hc = Hcope(D, policy, int(len(D)*0.4), 10)\n",
    "    for i in range(10):\n",
    "        print(\"step\",i)\n",
    "        theta = hc.sample_theta_c()\n",
    "        policy = FBSoftmax(4,2,1,0)\n",
    "        numEpisode = 100\n",
    "\n",
    "        environment = cp.Cartpole\n",
    "\n",
    "        t = CartPoleEvaluation(policy, environment)\n",
    "\n",
    "        mean_J_hat, D = t.run_policy(theta.reshape((2,5)), numEpisode)\n",
    "        print(\"theta: \", theta)\n",
    "        test_policy = FBSoftmax(num_state_features,num_actions,k,0)\n",
    "        test_policy.parameters = theta\n",
    "        ret = hc.test_pdis(test_policy)\n",
    "        print(\"pdis:\",ret)\n",
    "        print(\"Average J After:\"+ str(mean_J_hat))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
